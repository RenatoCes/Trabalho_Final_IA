{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RenatoCes/Trabalho_Final_IA/blob/Felipe/Reconhecimento_Facial_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOh-bBp5Ys2k",
        "outputId": "e7540c75-8904-4a6c-fe21-d2db23ae5c17"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python face_recognition fer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZE_x8pId4AD"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import face_recognition\n",
        "import numpy as np\n",
        "import os\n",
        "import base64\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import matplotlib.pyplot as plt\n",
        "from fer import FER  # Importando a biblioteca FER para reconhecimento de emoções\n",
        "\n",
        "# Carregar o classificador Haar Cascade\n",
        "alg = \"/content/haarcascade_frontalface_default.xml\"\n",
        "haar_cascade = cv2.CascadeClassifier(alg)\n",
        "\n",
        "# Função para codificar uma face usando face_recognition\n",
        "def encode_face(image):\n",
        "    if image is None:\n",
        "        return None\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    face_encodings = face_recognition.face_encodings(rgb_image)\n",
        "    if face_encodings:\n",
        "        return face_encodings[0]\n",
        "    return None\n",
        "\n",
        "# Carregar faces armazenadas\n",
        "stored_faces_dir = \"stored_faces\"\n",
        "known_faces = {}\n",
        "if not os.path.exists(stored_faces_dir):\n",
        "    os.makedirs(stored_faces_dir)\n",
        "\n",
        "for filename in os.listdir(stored_faces_dir):\n",
        "    if filename.endswith(\".jpg\"):\n",
        "        name = os.path.splitext(filename)[0]\n",
        "        stored_face = face_recognition.load_image_file(os.path.join(stored_faces_dir, filename))\n",
        "        encoding = encode_face(stored_face)\n",
        "        if encoding is not None:\n",
        "            known_faces[name] = encoding\n",
        "\n",
        "# Função para capturar uma imagem da webcam usando JavaScript\n",
        "def capture_image():\n",
        "    js = Javascript('''\n",
        "        async function captureImage() {\n",
        "            const video = document.createElement('video');\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "            video.srcObject = stream;\n",
        "            await new Promise((resolve) => {\n",
        "                video.onloadedmetadata = () => {\n",
        "                    resolve(video);\n",
        "                };\n",
        "            });\n",
        "            document.body.appendChild(video);\n",
        "            video.play();\n",
        "\n",
        "            // Create a canvas to capture the frame\n",
        "            const canvas = document.createElement('canvas');\n",
        "            canvas.width = video.videoWidth;\n",
        "            canvas.height = video.videoHeight;\n",
        "            const context = canvas.getContext('2d');\n",
        "            context.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
        "            \n",
        "            // Stop the video stream\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "            video.remove();\n",
        "\n",
        "            return canvas.toDataURL('image/jpeg', 1.0);\n",
        "        }\n",
        "        captureImage();\n",
        "    ''')\n",
        "    display(js)\n",
        "    img_str = eval_js('captureImage()')\n",
        "    img_data = img_str.split(',')[1]\n",
        "    nparr = np.frombuffer(base64.b64decode(img_data), np.uint8)\n",
        "    frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    return frame\n",
        "\n",
        "# Função para exibir a imagem capturada\n",
        "def display_image(image):\n",
        "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(rgb_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Função para verificar se a face está registrada e detectar emoções\n",
        "def check_face_and_emotion(frame):\n",
        "    if frame is None:\n",
        "        print(\"Erro: Imagem não capturada corretamente.\")\n",
        "        return\n",
        "\n",
        "    display_image(frame)  # Mostrar a imagem capturada\n",
        "\n",
        "    new_face_encoding = encode_face(frame)\n",
        "    if new_face_encoding is not None:\n",
        "        matches = face_recognition.compare_faces(list(known_faces.values()), new_face_encoding)\n",
        "        if any(matches):\n",
        "            match_index = matches.index(True)\n",
        "            match_name = list(known_faces.keys())[match_index]\n",
        "            print(f\"Face correspondente encontrada: {match_name}\")\n",
        "        else:\n",
        "            print(\"Nenhuma correspondência encontrada para esta face.\")\n",
        "    else:\n",
        "        print(\"Não foi possível codificar a face.\")\n",
        "    \n",
        "    # Detectar emoções\n",
        "    emotion_detector = FER(mtcnn=True)\n",
        "    emotions = emotion_detector.detect_emotions(frame)\n",
        "    if emotions:\n",
        "        dominant_emotion, score = emotion_detector.top_emotion(frame)\n",
        "        print(f\"Emoção detectada: {dominant_emotion} com confiança de {score:.2f}\")\n",
        "    else:\n",
        "        print(\"Nenhuma emoção detectada.\")\n",
        "\n",
        "# Função principal\n",
        "def main():\n",
        "    answer = input(\"Verificar se o seu rosto está registrado e detectar emoções? sim ou não? \").strip().lower()\n",
        "    if answer == \"não\":\n",
        "        print(\"Programa encerrado.\")\n",
        "    elif answer == \"sim\":\n",
        "        print(\"Ligando a câmera...\")\n",
        "        frame = capture_image()\n",
        "        print(\"Verificando face e emoções...\")\n",
        "        check_face_and_emotion(frame)\n",
        "    else:\n",
        "        print(\"Resposta inválida. Programa encerrado.\")\n",
        "\n",
        "# Executar o programa principal\n",
        "main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyML5BJJt80nDaPqSvh6b+9h",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
